---
title: "Building spatial databases"
linktitle: "5: Spatial databases in R"
date: "2021-10-06"
toc: yes
menu:
  example:
    parent: Examples
    weight: 5
type: docs
editor_options: 
  chunk_output_type: console
---
One of the strongest justifications for using `R` as a Geographic Information System (GIS) is the ability to construct complete, reproducible workflows from data entry and cleaning through analysis and visualization. Constructing databases that combine spatial and tabular data is at the core of those workflows. This example should help you practice some of the key database operations that you may encounter as you conduct your own analysis.

# Load the datasets and packages

The data we'll be using for today's example and assignment are a mix of spatial and tabular data. The tabular data come from the [Land-Grab University(LGU)](https://www.landgrabu.org/) data on parcel sales and the National Park Service's (NPS) [annual visitation data](https://irma.nps.gov/STATS/Reports/National) for NPS-operated units in the Intermountain and Pacific West regions. The spatial data is a shapefile from the US Protected Area Database (PADUS) and a shapefile containing the parcels described by the Land-Grab University parcel database. We'll load each of those here and take a look at features and fields in each dataset. __Remember that you'll need to change the paths to match your own data.__

```{r loadup}
library(sf)
library(tidyverse)


nps.visits.csv <- read_csv('/Users/matthewwilliamson/Downloads/session14/ParkVisits_2020IWPW.csv', skip = 3)
head(nps.visits.csv)

parcels.csv <- read_csv('/Users/matthewwilliamson/Downloads/session14/Parcels.csv')
head(parcels.csv)

regional.pas.sf <- read_sf('/Users/matthewwilliamson/Downloads/session14/reg_pas.shp')

regional.pas.sf

regional.parcels.sf <- read_sf('/Users/matthewwilliamson/Downloads/session14/Parcel_Polygons.shp')

regional.parcels.sf

```

# Check the geometries, understand the data

By now you know that before we get too far down the road, we want to check the geometries and projections of our spatial data. Let's do that here. We can use the `all()` function to check to see if all of the geometries are valid (i.e., `st_is_valid()` returns all `TRUE`).

```{r checkup}
all(st_is_valid(regional.pas.sf))
all(st_is_valid(regional.parcels.sf))

```

Unfortunately, they aren't all valid so we'll need to fix that here.

```{r validate}
regional.pas.valid <- st_make_valid(regional.pas.sf)
all(st_is_valid(regional.pas.valid)) #fixed it!

regional.parcels.valid <- st_make_valid(regional.parcels.sf)
all(st_is_valid(regional.parcels.valid)) #fixed it!

```

Now that we've gotten the geometries cleaned up, we need to make sure the two datasets align.

```{r project}
st_crs(regional.parcels.valid) == st_crs(regional.pas.valid) #Of course they dont

parcels.project <- regional.parcels.valid %>% 
  st_transform(., crs = st_crs(regional.pas.valid))

st_crs(parcels.project) == st_crs(regional.pas.valid) #fixed it!

```

# Where are we going?

We'd like to explore how visitation to a number of NPS units contrasts with the amount of money the US paid to purchase the land those units sit on. This means we'll need a dataset that contains: a) the total number of visits to each unit in 2020, b) the number of parcels that were sold within the present-day boundaries of those units, and c) the sum of the value paid for those units.

# Subset the data to suit our questions

I'm going to focus on Idaho for this example, so I'll subset the data for that here.
```{r filter}
id.parcels.csv <- parcels.csv %>% 
  filter(., Loc_State == "ID")

id.park.visits <- nps.visits.csv %>% 
  filter(., State == "ID")

id.pas.sf <- regional.pas.valid %>% 
  filter(., Stat_Nm == "ID")
```

# Joining with keypairs
You'll notice that the the LGU parcels dataset does not have many attributes and none that will let us use `filter` to reduce the dataset. That's because the bulk of the tabular data is stored in the `parcels.csv` file. We'll have to join them before we can subset them based on an attribute. Remember, that the `_join` commands return objects with the `class` of the first argument. 

```{r class1}
join1 <- left_join(id.parcels.csv, parcels.project, by = "MTRSA_LG")
class(join1)

join2 <- left_join(parcels.project, id.parcels.csv, by = "MTRSA_LG")
class(join2)
```

Given this, there are two ways we might approach joining and subsetting the data. 

```{r join}
id.parcels.sf <- left_join(parcels.project, id.parcels.csv, by = "MTRSA_LG") %>% 
  filter(., Loc_State == "ID")
class(id.parcels.sf)
nrow(id.parcels.sf)

id.parcels.sf2 <- inner_join(parcels.project, id.parcels.csv, by = "MTRSA_LG")
class(id.parcels.sf2)
nrow(id.parcels.sf2)
```


Okay, now all we need to do is get the visit data attached to the NPS. But before we can do that, we need to summarize the monthly visits into a total for the year.

```{r aggnp}

id.park.visits.ann <- id.park.visits %>% 
  group_by(UnitCode) %>% 
  summarise(., Total = sum(RecreationVisits))

id.pas.visits <- id.pas.sf %>% 
  inner_join(., id.park.visits.ann, by = c("Loc_Nm" = "UnitCode"))
```

# Joining based on location

Now we need to figure out how many parcels are within the orignal NPS unit boundaries. We'll use a spatial join (`st_join`) to make that happen. Remember, that we can use any of the binary predicates (`?geos_binary_pred`) to specify how we'd like to join the data. Here, I'll `st_is_within_distance` to illustrate.

```{r sj1}

parcels.in.pas <- st_join(id.pas.visits, id.parcels.sf, join=st_is_within_distance, dist=100000)
```

# Estimating the total amount paid

The result of selecting all of the parcels with 100000m of the NPS units is a data frame with 26 features (i.e., 26 parcels met the criteria). Now we need to calculate the total value (remember that we have to fix the currency to make it numeric).

```{r aggreg}
parcels.in.pas.sum <- parcels.in.pas %>% 
  group_by(Unit_Nm) %>% 
  summarise(., total = sum(readr::parse_number(US_Paid_for_Parcel), na.rm=TRUE))
```
